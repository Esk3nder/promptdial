# The "Prompt Engineering Stack"

*A layered, endâ€‘toâ€‘end system for writing worldâ€‘class prompts*

---

## Layer 0 â€” Model & Context Selection

1. **Pick a reasoningâ€‘grade model** (e.g., OpenAI o3, Claude Opus 4, Gemini 2.5 Pro).
2. **Load task context** (domain corpus, tool APIs, retrieval endpoints).

> Without this foundation, higherâ€‘layer tactics deliver diminishing returns.

---

## Layer 1 â€” Contractâ€‘First Intent Clarification

*(adapted from the "Intent Translator MAX" prompt)*

| Phase | What the model does | Your control signals |
| ----------------------- | ---------------------------------------------------------------------------- | ----------------------------- |
| **0 SILENT SCAN** | Lists unknown facts/constraints. | â€” |
| **1 CLARIFY LOOP** | Asks one targeted question until â‰¥ 95 % confidence. | Answer or type `RESET`. |
| **2 ECHO CHECK** | Returns singleâ€‘sentence summary + "âœ… YES / âŒ EDITS / ğŸ”§ BLUEPRINT / âš  RISK". | Confirm or request subâ€‘modes. |
| **3 BLUEPRINT** | Outlines key steps / Iâ€‘O schema. | Pause, tweak, lock. |
| **4 RISK** | Surfaces top 3 failure modes. | Mitigate or revise. |
| **5 BUILD & SELFâ€‘TEST** | Generates output, runs static checks. | Review deliverable. |

This handshake prevents the #1 failure modeâ€”misaligned intentâ€”before any heavy token spend.

---

## Layer 2 â€” Prompt Blueprint (Pâ€‘Iâ€‘Râ€‘O)

Structure every final prompt in **four slots**:

1. **Purpose** â€“ goal, audience, success criteria.
2. **Instructions** â€“ step order, guardrails/edgeâ€‘cases, fallback behaviour.
3. **Reference** â€“ data snippets, style guides, exemplar Qâ€‘A pairs.
4. **Output** â€“ format contract (JSON / Markdown / code), length/tone limits.

This mirrors the taxonomy in the Prompt Report's bestâ€‘practice guidelines.

---

## Layer 3 â€” Embedded Reasoning Drivers

*Attach the evidenceâ€‘based techniques inside the blueprint's **Instructions** block.*

| Technique | When to embed | Microâ€‘syntax |
| ----------------------------------- | --------------------------- | ---------------------------------------------------------- |
| **Selfâ€‘Consistency** | Knowledge & logic questions | "Generate 5 candidate answers; compare & emit consensus." |
| **Programâ€‘ofâ€‘Thought** | Math / data / code | "Write Python to compute â€¦ then return result." |
| **Planâ€‘Thenâ€‘Solve** | Multiâ€‘step tasks | "First produce a numbered plan, wait for âœ…, then execute." |
| **Retrievalâ€‘Augmented (RAG)** | Knowledgeâ€‘intensive tasks | "If fact needed, call SEARCH() and cite." |
| **Chain / Tree / Graphâ€‘ofâ€‘Thought** | Complex reasoning trees | "Think stepâ€‘byâ€‘step; branch if ambiguity > 0.2." |

---

## Layer 4 â€” Metaâ€‘Prompting Loops

Leverage the model to improve its own prompt:

1. **Selfâ€‘Improvement Loop** â€“ "Here's my current prompt â†’ suggest 3 upgrades â†’ apply best."
2. **Uncertainty Probe** â€“ "List ambiguous parts & missing info."
3. **Capability Discovery** â€“ "If unconstrained, how else could you solve this?"
4. **Explainâ€‘Yourâ€‘Work / Socratic Why** â€“ "Why did you choose approach X? What alternatives?"

Metaâ€‘prompting is a formally recognised technique in the literature.

---

## Layer 5 â€” Autoâ€‘Optimization (Optional)

When scale matters, wrap Layer 4 with automation:

* **APE / GrIPS / AutoPrompt** to generate, score and iterate prompt variants.
* Reinforcement or evolutionary search on task metrics.
* Keep a "prompt registry" with version tags, eval traces, and rollback keys.

---

## Layer 6 â€” Evaluation & Hardening

*Every prompt ships with its own testâ€‘harness.*

| Dimension | Method |
| --------------- | ------------------------------------------------------------------------------------------------ |
| **Correctness** | Fewâ€‘shot hidden tests, gold benchmarks, automatic selfâ€‘verification |
| **Robustness** | Canary phrases, negative examples, antiâ€‘jailbreak rules (see security section of Prompt Report) |
| **Calibration** | "Give answer + confidence 0â€‘1; if < 0.6, cite uncertainty source." |
| **Performance** | Token budget assertions in output block; static codeâ€‘lint for programâ€‘ofâ€‘thought. |

---

## Copyâ€‘Ready Master Template

```text
########################################
# PROMPT BLUEPRINT (Pâ€‘Iâ€‘Râ€‘O pattern) #
########################################

PURPOSE
â€¢ Mission: <<crisp objective>>
â€¢ Success criteria: <<measurable>>

INSTRUCTIONS
1. Contractâ€‘First handshake (CLARIFY â†’ ECHO â†’ YES/EDITS).
2. Planning: generate step list, wait for âœ…LOCK.
3. Execution: follow Programâ€‘ofâ€‘Thought (Python) or RAG as needed.
4. Selfâ€‘Consistency: produce N variants, reconcile â†’ final_answer.
5. Output must follow schema in OUTPUT section.
6. Guardrails:
â€“ If unable â†’ respond "UNCERTAIN".
â€“ Forbidden: â€¦ # security
â€“ Edgeâ€‘cases: â€¦ # domainâ€‘specific

REFERENCE
<<insert facts, style snippets, exemplar Qâ€‘A, API docs>>

OUTPUT
```json
{
"answer": "...",
"reasoning": "...",
"confidence": 0.0â€‘1.0
}
```

########################################
```

---

## Quickâ€‘Start Checklist

| âœ”ï¸ | Question |
|----|----------|
| â˜ | Have I picked a reasoningâ€‘grade model & loaded context? |
| â˜ | Did the CLARIFY loop hit 95 % confidence? |
| â˜ | Does the blueprint include Pâ€‘Iâ€‘Râ€‘O slots? |
| â˜ | Which reasoning driver(s) are embedded? |
| â˜ | Is a metaâ€‘prompting selfâ€‘improvement hook present? |
| â˜ | Are robustness tests & uncertainty outputs defined? |

---

## How to Use This Stack

1. **Start at Layer 1** with the Contractâ€‘First scaffold; lock intent.
2. **Fill Pâ€‘Iâ€‘Râ€‘O** using domain artefacts.
3. **Attach Layer 3 drivers** relevant to task type.
4. **Run meta loop** until the prompt scores â‰¥ pass on your eval harness.
5. **Snapshot & version**; monitor drift over time.

Execute these layers faithfully and you'll be writingâ€”by designâ€”the **best fucking prompts on the planet**.